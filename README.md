# COMPSCI-646-group
A Study of Hard Negative Samples on Reasoning-aware Retrieval Models

âš ï¸ **You should study Guide 2 before study Guide 1.** This is because Guide 2 generates hard negative queries and mines positive documents which will be used to generate hard negative documents in Guide 1.

## ðŸ“–Guide 1: generate negative samples using LLM

This guide will teach you how to generate negative samples using LLM.
* Step 1: create a new file '.env' under `negative_sample` folder. Enter
```
OPENAI_API_KEY = [The key]
```
Replace [The key] with an openai api key. You can use the one provided by Jianming. It's under our Notion page -> Misc. 
* Step 2: from `negative_sample`, run
```
pip install -r requirements.txt
```
* Step 3: from `negative_sample`, run
```
python supplement_negative_passage.py --input_file data/sample.jsonl
```
You should see a newly generated file called `sample_generated_negative.jsonl`. That contains a new column `neg`, which is the negative sample.

## ðŸ“–Guide 2: generate negative samples using LLM 
âš ï¸ I strongly recommend you to install everything in a dedicated python environment. (>= python 3.10)

This is the newer guide that teaches you how to generate better negative samples using LLM.
* Step 1: create a new file '.env' under `synthetic_data_generation` folder. Enter
```
OPENAI_API_KEY = [The key]
```
Replace [The key] with an openai api key. You can use the one provided by Jianming. It's under our Notion page -> Misc. 

* Step 2: from `synthetic_data_generation`, run
```
pip install -r requirements.txt
```

* Step 3: 

if you are using Linux, run (not tested):
```
bash setup_java_linux.sh
```
if you are using Windows, run:
```
bash setup_java_winos.sh
```
It's highly likely that you will be encountering errors. After solving them you should have everything installed and you can proceed to running the commands introduced below.

## ðŸ“•Generate the queries based on the documents from the datastore of BRIGHT

Below is a provided template from ReasonIR. 
```
python -m doc_to_query --model_id $MODEL --queries_per_doc $queries_per_doc --num_docs $num_docs --subject $TASK  --output_dir $output_dir --filter fineweb --prompt_id $prompt_id
```

Example usage:
```
python -m doc_to_query --model_id gpt-4o --queries_per_doc 3 --num_docs 1 --subject "biology" --output_dir ./outputs/doc_to_query/ --filter fineweb --prompt_id "hq_gen"
```
This command generates three tuples of (hard query, positive document, negative document) on a random query about subject biology. Filtered by fineweb (discarding low-score documents and keep 1 in the end). "hq_gen" stands for hard-query generation.

> ðŸ’¡ Notice that positive document and negative document are generated by BM25 and we won't be using the negative document result from this step. The hard negative document will be generated by supplement_negative_passage.py

### Command arguments
| argument | type | default |
| -------- | ---- | ------- |
| --mode | str | None |
| --model_id | str | gpt-4o |
| --dataset | str | bright |
| --subject | str | None |
| --queries_per_doc | int | 3 |
| --num_docs | int | None |
| --filter | str | None |
| --data_path | str | **Don't change** |
| --output_dir | str | **Don't change** |
| --cache_dir | str | **Don't change** |
| --prompt_id | str | hq_gen |
| --temperature | float | 0 |
| --top_p | float | 0 |

## ðŸ“—Generate the rewritten queries with reasoning given the queries

Command template:
```
python -m generate_reasoning --model_id $MODEL --num_docs $num_docs --subject $TASK --base_dir $output_dir --prompt_id $prompt_id
```

## ðŸ“˜Batched version

```
python -m doc_to_query_batch --model_id $MODEL --queries_per_doc $queries_per_doc --num_docs $num_docs --subject $TASK --output_dir $output_dir --filter fineweb --prompt_id $prompt_id
```

```
python -m doc_to_query_batch --model_id $MODEL --queries_per_doc $queries_per_doc --num_docs $num_docs --subject $TASK --output_dir $output_dir --filter fineweb --prompt_id $prompt_id --gather_results
```

```
python -m generate_reasoning_batch --model_id $MODEL --num_docs $num_docs --subject $TASK --base_dir $output_dir --prompt_id $prompt_id
```

```
python -m generate_reasoning_batch --model_id $MODEL --num_docs $num_docs --subject $TASK --base_dir $output_dir --prompt_id $prompt_id --gather_results
```
